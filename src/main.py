# -*- coding: utf-8 -*-

import os
import json
import telegram
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import Application, CallbackQueryHandler, ContextTypes
from telegram.constants import ParseMode
import pyalex
from dotenv import load_dotenv
import asyncio
import re
import yaml
from glob import glob

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ---
load_dotenv()

# --- –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
TELEGRAM_MOD_CHANNEL_ID = os.getenv('TELEGRAM_MODERATION_CHANNEL_ID')
TELEGRAM_PUB_CHANNEL_ID = os.getenv('TELEGRAM_PUBLIC_CHANNEL_ID')
pyalex.config.email = os.getenv('OPENALEX_EMAIL')

# ===================================================================
# –ú–û–î–£–õ–¨ 1: –£–¢–ò–õ–ò–¢–´ –ò –•–†–ê–ù–ò–õ–ò–©–ï
# (–∑–¥–µ—Å—å –≤—Å–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ===================================================================
def reconstruct_abstract(inverted_index: dict) -> str:
    if not inverted_index: return ""
    try:
        max_pos = max(max(positions) for positions in inverted_index.values() if positions)
        abstract_list = [''] * (max_pos + 1); [abstract_list.__setitem__(pos, word) for word, positions in inverted_index.items() for pos in positions]
        return ' '.join(abstract_list)
    except (ValueError, TypeError): return ""

def strip_html_tags(text: str) -> str:
    if not text: return ""
    return re.sub('<[^<]+?>', '', text)

class StateManager:
    def __init__(self, filepath="processed_ids.json"): self.filepath = filepath
    def load_processed_ids(self) -> set:
        if not os.path.exists(self.filepath): return set()
        try:
            with open(self.filepath, 'r', encoding='utf-8') as f: return set(json.load(f))
        except (json.JSONDecodeError, FileNotFoundError): return set()
    def save_processed_ids(self, ids: set):
        with open(self.filepath, 'w', encoding='utf-8') as f: json.dump(list(ids), f, indent=4)

class Translator:
    def translate(self, text: str, target_language: str = "ru") -> str:
        return text

# ===================================================================
# –ú–û–î–£–õ–¨ 2: –°–ë–û–† –î–ê–ù–ù–´–• –ò –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–Ø
# ===================================================================

class Summarizer:
    def summarize(self, paper: dict) -> str:
        print(f"-> [Summarizer] –°–æ–∑–¥–∞—é –Ω–æ–≤–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞—Ç—å–∏: '{paper['title'][:30]}...'")
        title = paper['title']
        problem = "–ü—Ä–æ–±–ª–µ–º–∞ XYZ, –∏–∑—É—á–∞–µ–º–∞—è –≤ —Å—Ç–∞—Ç—å–µ."
        method = "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –º–µ—Ç–æ–¥ ABC."
        sample = "–í—ã–±–æ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–ª–∞ –∏–∑ N —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤."
        authors_conclusion = "–ê–≤—Ç–æ—Ä—ã –ø—Ä–∏—à–ª–∏ –∫ –≤—ã–≤–æ–¥—É, —á—Ç–æ..."
        pdf_url = paper['pdf_url']
        summary_text = (f"<b>{title}</b>\n\n<b>–ü—Ä–æ–±–ª–µ–º–∞:</b> {problem}\n<b>–ú–µ—Ç–æ–¥:</b> {method}\n<b>–í—ã–±–æ—Ä–∫–∞:</b> {sample}\n<b>–í—ã–≤–æ–¥—ã:</b> {authors_conclusion}\n\n<a href='{pdf_url}'>üîó –°—Å—ã–ª–∫–∞ –Ω–∞ PDF</a>")
        return summary_text

class OpenAlexFetcher:
    """
    –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ –¥–ª—è OpenAlex.
    –¢–µ–ø–µ—Ä—å —É–º–µ–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∏ —Å "concepts", –∏ —Å "topics", –∏ —Å "type".
    """
    def fetch(self, config: dict) -> list:
        print(f"--- [OpenAlex] –ò—â—É —Å—Ç–∞—Ç—å–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ---")
        try:
            query = pyalex.Works()

            # --- –ö–õ–Æ–ß–ï–í–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï ---
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ç–∏–ø—É –¥–æ–∫—É–º–µ–Ω—Ç–∞. pyalex –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–∏–ø–æ–≤ —á–µ—Ä–µ–∑ '|'.
            if config.get('document_types'):
                query = query.filter(type="|".join(config['document_types']))
            # --- –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø ---

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ Concepts –∏–ª–∏ Topics
            if config.get('primary_concepts'): query = query.filter(concepts={'id': "|".join(config['primary_concepts'])})
            if config.get('primary_topics'): query = query.filter(topics={'id': "|".join(config['primary_topics'])})
            if config.get('domain_concepts'): query = query.filter(concepts={'id': "|".join(config['domain_concepts'])})
            if config.get('domain_topics'): query = query.filter(topics={'id': "|".join(config['domain_topics'])})
            if config.get('exclude_concepts'):
                for concept_id in config['exclude_concepts']: query = query.filter(concepts={'id': f"!{concept_id}"})
            if config.get('exclude_topics'):
                for topic_id in config['exclude_topics']: query = query.filter(topics={'id': f"!{topic_id}"})
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            query = query.filter(
                publication_year=f">{config['start_year'] - 1}", is_oa=True, has_pdf_url=True
            ).sort(publication_date="desc").select([
                'id', 'title', 'authorships', 'publication_year', 
                'best_oa_location', 'abstract_inverted_index'
            ])
            
            raw_papers = query.get(per_page=config.get('fetch_limit', 25))
            return [self._normalize(paper) for paper in raw_papers]
            
        except Exception as e:
            print(f"‚ùå [OpenAlex] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
            return []
            
    def _normalize(self, paper: dict) -> dict:
        return {"id": paper.get('id'), "title": paper.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è'), "abstract": reconstruct_abstract(paper.get('abstract_inverted_index')), "authors": [a.get('author', {}).get('display_name') for a in paper.get('authorships', [])], "year": paper.get('publication_year'), "pdf_url": paper.get('best_oa_location', {}).get('pdf_url'), "source_name": "OpenAlex"}

class FetcherService:
    # ... (–∫–æ–¥ —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Å–∞ –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è)
    def __init__(self): self.fetchers = {"openalex": OpenAlexFetcher()}
    def run(self) -> list:
        print("\n=== –ó–ê–ü–£–°–ö –°–ï–†–í–ò–°–ê –°–ë–û–†–ê –î–ê–ù–ù–´–• ==="); all_new_papers = []
        for config_path in glob("sources/*.yaml"):
            with open(config_path, 'r', encoding='utf-8') as f: config = yaml.safe_load(f)
            if not config.get('enabled'): continue
            fetcher_type = config.get('fetcher_type')
            if fetcher_type in self.fetchers: all_new_papers.extend(self.fetchers[fetcher_type].fetch(config))
        print(f"–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω. –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_new_papers)} —Å—Ç–∞—Ç–µ–π.")
        return all_new_papers

# ===================================================================
# –ú–û–î–£–õ–¨ 3: –õ–û–ì–ò–ö–ê –ë–û–¢–ê
# (–∑–¥–µ—Å—å –≤—Å–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ===================================================================

async def send_for_triage(context: ContextTypes.DEFAULT_TYPE):
    fetcher_service = FetcherService()
    state_manager = StateManager()
    all_papers = await asyncio.to_thread(fetcher_service.run)
    processed_ids = state_manager.load_processed_ids()
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(processed_ids)} —Ä–∞–Ω–µ–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö ID.")
    if 'papers_in_progress' not in context.bot_data: context.bot_data['papers_in_progress'] = {}
    new_papers_found = 0
    for paper in sorted(all_papers, key=lambda p: p.get('year', 0)):
        if paper['id'] in processed_ids or not paper.get('pdf_url'): continue
        context.bot_data['papers_in_progress'][paper['id']] = paper
        keyboard = [[InlineKeyboardButton("üëç –í —Ä–∞–±–æ—Ç—É", callback_data=f"triage_accept_{paper['id']}"), InlineKeyboardButton("üëé –û—Ç–∫–ª–æ–Ω–∏—Ç—å", callback_data=f"triage_reject_{paper['id']}")]]
        reply_markup = InlineKeyboardMarkup(keyboard)
        message_text = f"<b>{paper['title']}</b>\n\n<i>{strip_html_tags(paper['abstract'])[:700]}...</i>\n\n<a href='{paper['pdf_url']}'>üîó PDF</a>"
        await context.bot.send_message(chat_id=TELEGRAM_MOD_CHANNEL_ID, text=message_text, parse_mode=ParseMode.HTML, reply_markup=reply_markup)
        processed_ids.add(paper['id'])
        new_papers_found += 1
        await asyncio.sleep(2)
    state_manager.save_processed_ids(processed_ids)
    print(f"–û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é: {new_papers_found} —Å—Ç–∞—Ç–µ–π.")

async def moderation_callback_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    stage, action, paper_id = query.data.split("_", 2)
    original_message = query.message
    if stage == "triage":
        await original_message.delete()
        if action == "accept":
            paper_data = context.bot_data['papers_in_progress'].get(paper_id)
            if not paper_data: return
            summarizer = Summarizer()
            summary_text = summarizer.summarize(paper_data)
            keyboard = [[InlineKeyboardButton("‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å", callback_data=f"publish_accept_{paper_id}"), InlineKeyboardButton("‚úèÔ∏è –û—Ç–∫–ª–æ–Ω–∏—Ç—å", callback_data=f"publish_reject_{paper_id}")]]
            reply_markup = InlineKeyboardMarkup(keyboard)
            await context.bot.send_message(chat_id=TELEGRAM_MOD_CHANNEL_ID, text=summary_text, parse_mode=ParseMode.HTML, reply_markup=reply_markup)
    elif stage == "publish":
        await original_message.delete()
        if action == "accept":
            await context.bot.send_message(chat_id=TELEGRAM_PUB_CHANNEL_ID, text=original_message.text_html, parse_mode=ParseMode.HTML)
            print(f"‚úÖ –ù–æ–≤–æ—Å—Ç—å {paper_id} –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞.")
            if paper_id in context.bot_data['papers_in_progress']:
                del context.bot_data['papers_in_progress'][paper_id]

# ===================================================================
# –ì–õ–ê–í–ù–´–ô –ó–ê–ü–£–°–ö
# (–∑–¥–µ—Å—å –≤—Å–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
# ===================================================================
def main():
    if not all([TELEGRAM_BOT_TOKEN, TELEGRAM_MOD_CHANNEL_ID, TELEGRAM_PUB_CHANNEL_ID]):
        print("‚ùå –û—à–∏–±–∫–∞: –ù–µ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–¥–∞–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ .env —Ñ–∞–π–ª.")
        return
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()
    application.add_handler(CallbackQueryHandler(moderation_callback_handler))
    job_queue = application.job_queue
    job_queue.run_repeating(send_for_triage, interval=14400, first=10)
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–µ–∂–∏–º–µ. –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—á–Ω–µ—Ç—Å—è —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥...")
    application.run_polling()

if __name__ == "__main__":
    main()
