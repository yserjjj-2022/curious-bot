# -*- coding: utf-8 -*-

import os
import json
import yaml
import pyalex
import re
from glob import glob
from datetime import datetime

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ---
from dotenv import load_dotenv
load_dotenv()
pyalex.config.email = os.getenv('OPENALEX_EMAIL', 'user@example.com')

def normalize_id(openalex_id):
    if not openalex_id: return None
    return openalex_id.split('/')[-1]

def normalize_title(title):
    if not title: return ""
    return re.sub(r'[^a-z0-9]', '', title.lower())

def clean_title_for_ascii_check(text):
    if not text: return ""
    text = text.replace('‚Äú', '"').replace('‚Äù', '"').replace('‚Äò', "'").replace('‚Äô', "'")
    text = text.replace('‚Äî', '-').replace('‚Äì', '-')
    text = re.sub(r'\s+', ' ', text).strip()
    return re.sub('<[^<]+?>', '', text)

def is_likely_english(text, threshold=0.003):
    if not text: return True
    cleaned_text = clean_title_for_ascii_check(text)
    if len(cleaned_text) == 0: return True
    non_ascii_chars = sum(1 for char in cleaned_text if not char.isascii())
    return (non_ascii_chars / len(cleaned_text)) < threshold

def chunk_list(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

def analyze_source(config: dict):
    config_name = os.path.basename(config.get('config_path', 'default.yaml'))
    print(f"\n--- –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∏—Å—Ç–æ—á–Ω–∏–∫: {config_name} ---")
    try:
        all_results_pool = []
        seen_ids = set()
        total_topics = config.get('topics', [])
        total_available_estimate = 0
        
        # --- –ù–û–í–ê–Ø –õ–û–ì–ò–ö–ê: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–º ---
        topic_chunks = list(chunk_list(total_topics, 7))
        print(f"–†–∞–∑–±–∏–≤–∞—é {len(total_topics)} —Ç–µ–º –Ω–∞ {len(topic_chunks)} –∑–∞–ø—Ä–æ—Å–æ–≤...")

        for i, chunk in enumerate(topic_chunks):
            print(f"  -> –í—ã–ø–æ–ª–Ω—è—é –∑–∞–ø—Ä–æ—Å {i+1}/{len(topic_chunks)} –¥–ª—è {len(chunk)} —Ç–µ–º...")
            query = pyalex.Works()
            
            if config.get('search_in_fields'):
                for field, search_term in config['search_in_fields'].items():
                    query = query.filter(**{field: search_term})
            if config.get('language'): query = query.filter(language=config.get('language'))
            if config.get('publication_year'): query = query.filter(publication_year=config['publication_year'])
            if config.get('document_types'): query = query.filter(type="|".join(config['document_types']))
            query = query.filter(topics={'id': "|".join(chunk)})
            query = query.sort(publication_date="desc")
            
            # --- –ù–û–í–û–ï: –°—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–∞–Ω–∫–∞ ---
            try:
                chunk_count = query.count()
                print(f"    ...–Ω–∞–π–¥–µ–Ω–æ {chunk_count} —Ä–∞–±–æ—Ç.")
                total_available_estimate += chunk_count
            except Exception as count_e:
                print(f"    ...–Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ—Å—á–∏—Ç–∞—Ç—å —Ä–∞–±–æ—Ç—ã –¥–ª—è —ç—Ç–æ–≥–æ —á–∞–Ω–∫–∞: {count_e}")
            # ---------------------------------------------------

            select_fields = ['id', 'display_name', 'publication_year', 'publication_date', 'type', 'topics', 'language']
            chunk_results = query.select(select_fields).get(per_page=50) # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —â–µ–¥—Ä–æ–µ –∫–æ–ª-–≤–æ –∏–∑ –∫–∞–∂–¥–æ–≥–æ

            for paper in chunk_results:
                if paper.get('id') not in seen_ids:
                    all_results_pool.append(paper)
                    seen_ids.add(paper.get('id'))
        
        print(f"\n‚úÖ –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã.")
        print(f"   –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–∞–±–æ—Ç (—Å—É–º–º–∞ –ø–æ —á–∞–Ω–∫–∞–º): ~{total_available_estimate}")
        print(f"   –í—Å–µ–≥–æ –ø–æ–ª—É—á–µ–Ω–æ {len(all_results_pool)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π –æ—Ç API. –ù–∞—á–∏–Ω–∞—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫—É –∏ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É...")

        # --- –õ–æ–∫–∞–ª—å–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ---
        all_results_pool.sort(key=lambda x: x.get('publication_date', '1900-01-01'), reverse=True)
        
        analyzed_articles = []
        seen_normalized_titles = set()
        fetch_limit = config.get('fetch_limit', 50)

        for paper in all_results_pool:
            title = paper.get('display_name')
            if not title: continue
            
            target_language = config.get('language')
            if target_language and paper.get('language') and paper.get('language') != target_language: continue
            if target_language == 'en' and not is_likely_english(title): continue
            
            normalized = normalize_title(title)
            if normalized in seen_normalized_titles: continue
            seen_normalized_titles.add(normalized)
            
            paper_topics_raw = paper.get('topics', [])
            paper_topics_ids = {normalize_id(t['id']) for t in paper_topics_raw if t and t.get('id')}
            
            analyzed_articles.append({
                'id': paper.get('id'), 'title': title, 'year': paper.get('publication_year'),
                'type': paper.get('type'), 'matched_topics': [t for t in total_topics if t in paper_topics_ids]
            })
            if len(analyzed_articles) >= fetch_limit: break
        
        results_fetched = len(analyzed_articles)
        print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (—Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö, –Ω–∞ –Ω—É–∂–Ω–æ–º —è–∑—ã–∫–µ): {results_fetched}")
        
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        output_filename = f"debug__{config_name.replace('.yaml', '')}__{timestamp}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump({'analysis_info': {'config_file': config_name, 'timestamp': timestamp, 'total_available_estimate': f"~{total_available_estimate}", 'total_fetched_from_api': len(all_results_pool), 'final_results_count': results_fetched}, 'analyzed_articles': analyzed_articles}, f, indent=2, ensure_ascii=False)
        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_filename}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
        import traceback
        traceback.print_exc()

def main():
    print("=== –ó–ê–ü–£–°–ö –û–¢–õ–ê–î–û–ß–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê (v29, —Å –Ω–∞–¥–µ–∂–Ω–æ–π –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π) ===")
    base_config_path = 'sources/_base.yaml'
    try:
        with open(base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f) or {}
    except FileNotFoundError: base_config = {}
    
    source_configs = glob("sources/[!_]*.yaml")
    for config_path in source_configs:
        try:
            with open(config_path, 'r', encoding='utf-8') as f: srez_config = yaml.safe_load(f)
            if srez_config and srez_config.get('enabled'):
                final_config = {**base_config, **srez_config}
                final_config['topics'] = list(set(base_config.get('core_topics', []) + srez_config.get('topics', [])))
                final_config['config_path'] = config_path
                analyze_source(final_config)
        except Exception as e: print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path}: {e}")

if __name__ == "__main__":
    main()
